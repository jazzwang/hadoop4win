#!/bin/bash
mkdir -p /lib/jvm
mkdir -p /opt
DRIVE=$(cygpath -w / | sed 's#:*##')

if [ ! -x /lib/jvm/jdk* ]; then
  echo "Installing Sun JDK ...."
  unzip /usr/src/jdk*.zip -d /lib/jvm/
fi

if [ -x /apt-cyg ]; then
	mv /apt-cyg /bin/apt-cyg
	chmod a+x /bin/apt-cyg
fi

if [ ! -x /opt/hadoop ]; then
  echo "Installing Hadoop ...."
  tar zxvf /usr/src/hadoop-*.tar.gz -C /opt/
  mv /opt/hadoop-* /opt/hadoop
  cp -aRp /opt/hadoop/conf /opt/hadoop/conf-local
  cp -aRp /opt/hadoop/conf /opt/hadoop/conf-full
  mv /opt/hadoop/conf/ /opt/hadoop/conf-pseudo
  ln -s /opt/hadoop/conf-pseudo /opt/hadoop/conf
  cat > /opt/hadoop/conf/core-site.xml << EOF
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <name>fs.default.name</name>
    <value>hdfs://localhost:9000</value>
  </property>
  <property>
    <name>hadoop.tmp.dir</name>
    <value>/var/hadoop/hadoop-\${user.name}</value>
  </property>
</configuration>
EOF
  cat > /opt/hadoop/conf-full/core-site.xml << EOF
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <name>fs.default.name</name>
    <value>hdfs://$(hostname):9000</value>
  </property>
  <property>
    <name>hadoop.tmp.dir</name>
    <value>/var/hadoop/hadoop-\${user.name}</value>
  </property>
</configuration>
EOF
  cat > /opt/hadoop/conf/hdfs-site.xml << EOF
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>
</configuration>
EOF
  cp -aRp /opt/hadoop/conf/hdfs-site.xml /opt/hadoop/conf-full/hdfs-site.xml
  cat > /opt/hadoop/conf/mapred-site.xml << EOF
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <name>mapred.job.tracker</name>
    <value>localhost:9001</value>
  </property>
</configuration>
EOF
  cat > /opt/hadoop/conf-full/mapred-site.xml << EOF
<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
  <property>
    <name>mapred.job.tracker</name>
    <value>$(hostname):9001</value>
  </property>
</configuration>
EOF
fi

export JAVA_HOME=$(dirname /lib/jvm/jdk*/bin/)
echo "export JAVA_HOME=$JAVA_HOME" >> /opt/hadoop/conf/hadoop-env.sh
# HADOOP_HOME is depricated since 1.0.3(?)
#echo "export HADOOP_HOME=/opt/hadoop" >> /opt/hadoop/conf/hadoop-env.sh
echo "export HADOOP_HOME=/opt/hadoop" >> /etc/profile
echo "export HADOOP_CLASSPATH=/opt/hadoop" >> /opt/hadoop/conf/hadoop-env.sh
echo "export HADOOP_CONF_DIR=/opt/hadoop/conf" >> /opt/hadoop/conf/hadoop-env.sh
echo "export HADOOP_HEAPSIZE=128" >> /opt/hadoop/conf/hadoop-env.sh
echo "export JAVA_HOME=$JAVA_HOME" >> /etc/profile
echo "export PATH=/opt/hadoop/bin/:$JAVA_HOME/bin:\$PATH" >> /etc/profile
cd $JAVA_HOME
chmod a+x `find . -name "*.exe"`
chmod a+x `find . -name "*.dll"`
cd /opt/hadoop
echo "Formating NameNode ......"
/opt/hadoop/bin/hadoop namenode -format
source /etc/profile
ln -s /cygdrive/${DRIVE}/var/hadoop /var/hadoop
# 12-06-09: for windows hadoop cluster, 
#           we could not create datanode folder
#           for the first time.
#echo "Start Hadoop for the first time ......"
#/bin/start-hadoop-daemon
#echo "Leaving Safe Mode  ......"
#/opt/hadoop/bin/hadoop dfsadmin -safemode leave
#echo "Create tmp direcotry for default user ......"
#/opt/hadoop/bin/hadoop fs -mkdir tmp
echo "/bin/stop-hadoop" > ~/.bash_logout
echo "/bin/stop-hadoop" > /etc/skel/.bash_logout
## Setup Chinese Environment
## Ref: http://irw.ncut.edu.tw/peterju/cygwin.html
## Ref: http://hyperrate.com/thread.php?tid=15827
cat >> /etc/skel/.bashrc << EOF
stty cs8 -istrip
stty pass8
export LANG=zh_TW.UTF-8
export LC_CTYPE=zh_TW.UTF-8
alias ls="ls --show-control-chars"
EOF
cat >> /etc/skel/.inputrc << EOF
set meta-flag on
set input-meta on
set convert-meta off
set output-meta on 
EOF
#echo "Stoping Hadoop ......"
#/bin/stop-hadoop
